# T√ÇCHE 2 - SURVEILLANCE D'ALERTES ET R√âPONSE AUX INCIDENTS

![Status](https://img.shields.io/badge/Status-Active-green)
![Duration](https://img.shields.io/badge/Dur√©e-7%20jours-blue)
![Difficulty](https://img.shields.io/badge/Difficult√©-Avanc√©-red)
![Version](https://img.shields.io/badge/Version-1.0-brightgreen)

**D√©p√¥t GitHub**: `FUTURE_CS_02`  
**Dur√©e estim√©e**: 7 jours  

## üõ†Ô∏è Technologies & Outils

![ELK Stack](https://img.shields.io/badge/ELK%20Stack-8.8.0-orange)
![Elasticsearch](https://img.shields.io/badge/Elasticsearch-8.8.0-yellow)
![Logstash](https://img.shields.io/badge/Logstash-8.8.0-green)
![Kibana](https://img.shields.io/badge/Kibana-8.8.0-purple)
![Splunk](https://img.shields.io/badge/Splunk-Enterprise-blue)
![Python](https://img.shields.io/badge/Python-3.8+-blue)
![Docker](https://img.shields.io/badge/Docker-Latest-blue)

---

## üìã OVERVIEW DE LA T√ÇCHE

![Security](https://img.shields.io/badge/Domain-Cybersecurity-red)
![SIEM](https://img.shields.io/badge/Type-SIEM-orange)
![SOC](https://img.shields.io/badge/Operations-SOC-purple)

### üéØ Objectifs
- ![Config](https://img.shields.io/badge/-Configuration-lightgrey) Configurer un syst√®me SIEM (Security Information and Event Management)
- ![Analysis](https://img.shields.io/badge/-Analyse-lightblue) Analyser des logs de s√©curit√© en temps r√©el
- ![Classification](https://img.shields.io/badge/-Classification-yellow) Classifier les incidents de s√©curit√© par niveau de criticit√©
- ![Playbooks](https://img.shields.io/badge/-Playbooks-green) D√©velopper des playbooks de r√©ponse aux incidents
- ![Dashboard](https://img.shields.io/badge/-Dashboard-purple) Cr√©er des dashboards de surveillance

### üöÄ Comp√©tences d√©velopp√©es
- ![Log Analysis](https://img.shields.io/badge/Skill-Log%20Analysis-blue)
- ![SIEM Operations](https://img.shields.io/badge/Skill-SIEM%20Operations-green)
- ![Incident Response](https://img.shields.io/badge/Skill-Incident%20Response-red)
- ![SOC Operations](https://img.shields.io/badge/Skill-SOC%20Operations-purple)
- ![Threat Detection](https://img.shields.io/badge/Skill-Threat%20Detection-orange)
- ![Security Monitoring](https://img.shields.io/badge/Skill-Security%20Monitoring-yellow)

---

## üõ†Ô∏è PHASE 1: CONFIGURATION DE L'ENVIRONNEMENT SIEM

![Phase](https://img.shields.io/badge/Phase-1-blue)
![Infrastructure](https://img.shields.io/badge/Type-Infrastructure-green)

### 1.1 Installation ELK Stack avec Docker

![Docker Compose](https://img.shields.io/badge/Config-Docker%20Compose-blue)
![Status](https://img.shields.io/badge/Status-Ready-green)

```yaml
# docker-compose.yml
version: '3.8'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - xpack.security.transport.ssl.enabled=false
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
      - ./config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
    networks:
      - elk

  logstash:
    image: docker.elastic.co/logstash/logstash:8.8.0
    container_name: logstash
    ports:
      - "5044:5044"
      - "5000:5000/tcp"
      - "5000:5000/udp"
      - "9600:9600"
    volumes:
      - ./config/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
      - ./logs:/usr/share/logstash/logs:ro
      - ./config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
    environment:
      LS_JAVA_OPTS: "-Xmx512m -Xms512m"
    networks:
      - elk
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
      SERVER_NAME: kibana
      SERVER_HOST: "0.0.0.0"
    volumes:
      - ./config/kibana.yml:/usr/share/kibana/config/kibana.yml:ro
    networks:
      - elk
    depends_on:
      - elasticsearch

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.8.0
    container_name: filebeat
    user: root
    volumes:
      - ./config/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - ./logs:/var/log/security:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - elk
    depends_on:
      - elasticsearch
      - logstash

volumes:
  elasticsearch_data:

networks:
  elk:
    driver: bridge
```

### 1.2 Configuration Logstash

![Logstash](https://img.shields.io/badge/Component-Logstash-green)
![Processing](https://img.shields.io/badge/Type-Log%20Processing-orange)
![Security](https://img.shields.io/badge/Focus-Security%20Detection-red)

```ruby
# config/logstash.conf
input {
  beats {
    port => 5044
  }
  
  file {
    path => "/usr/share/logstash/logs/apache_access.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => "plain"
    tags => ["apache", "access"]
  }
  
  file {
    path => "/usr/share/logstash/logs/auth.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    tags => ["system", "auth"]
  }
  
  file {
    path => "/usr/share/logstash/logs/firewall.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    tags => ["network", "firewall"]
  }
}

filter {
  # Filtres pour logs Apache
  if "apache" in [tags] {
    grok {
      match => { 
        "message" => "%{COMBINEDAPACHELOG}" 
      }
    }
    
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    
    mutate {
      convert => { "response" => "integer" }
      convert => { "bytes" => "integer" }
    }
    
    # D√©tection d'attaques communes
    if [request] =~ /(?i)(union|select|script|alert|javascript|<|>)/ {
      mutate {
        add_tag => ["suspicious", "potential_attack"]
        add_field => { "threat_level" => "high" }
      }
    }
    
    # Classification des codes de r√©ponse
    if [response] >= 400 and [response] < 500 {
      mutate {
        add_tag => ["client_error"]
        add_field => { "severity" => "medium" }
      }
    } else if [response] >= 500 {
      mutate {
        add_tag => ["server_error"]
        add_field => { "severity" => "high" }
      }
    }
  }
  
  # Filtres pour logs d'authentification
  if "auth" in [tags] {
    grok {
      match => { 
        "message" => "%{SYSLOGTIMESTAMP:timestamp} %{HOSTNAME:hostname} %{WORD:service}(?:\[%{POSINT:pid}\])?: %{GREEDYDATA:auth_message}" 
      }
    }
    
    # D√©tection de tentatives de brute force
    if "Failed password" in [message] {
      grok {
        match => {
          "auth_message" => "Failed password for (?<failed_user>\w+) from %{IP:source_ip} port %{INT:source_port}"
        }
      }
      mutate {
        add_tag => ["failed_login", "brute_force_attempt"]
        add_field => { "threat_level" => "high" }
        add_field => { "severity" => "high" }
      }
    }
    
    # D√©tection de connexions suspectes
    if "Accepted password" in [message] {
      grok {
        match => {
          "auth_message" => "Accepted password for (?<successful_user>\w+) from %{IP:source_ip} port %{INT:source_port}"
        }
      }
      mutate {
        add_tag => ["successful_login"]
        add_field => { "severity" => "info" }
      }
    }
  }
  
  # Filtres pour logs firewall
  if "firewall" in [tags] {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{WORD:action} %{IP:source_ip}:%{INT:source_port} -> %{IP:dest_ip}:%{INT:dest_port} %{WORD:protocol}"
      }
    }
    
    if [action] == "DENY" or [action] == "DROP" {
      mutate {
        add_tag => ["blocked_connection"]
        add_field => { "severity" => "medium" }
      }
    }
    
    # D√©tection de scans de ports
    if [dest_port] in ["22", "23", "3389", "445", "135"] {
      mutate {
        add_tag => ["port_scan", "reconnaissance"]
        add_field => { "threat_level" => "medium" }
      }
    }
  }
  
  # Enrichissement GeoIP
  if [source_ip] {
    geoip {
      source => "source_ip"
      target => "geoip"
    }
  }
  
  # Ajout de m√©tadonn√©es
  mutate {
    add_field => { "[@metadata][index_name]" => "security-logs-%{+YYYY.MM.dd}" }
    add_field => { "event_processed_time" => "%{@timestamp}" }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index_name]}"
    template_name => "security_logs"
    template => "/usr/share/logstash/templates/security_template.json"
  }
  
  # Output conditionnel pour alertes critiques
  if [threat_level] == "high" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "security-alerts-%{+YYYY.MM.dd}"
    }
  }
  
  stdout { 
    codec => rubydebug 
  }
}
```

### 1.3 Configuration Kibana

![Kibana](https://img.shields.io/badge/Component-Kibana-purple)
![Visualization](https://img.shields.io/badge/Type-Visualization-blue)

```yaml
# config/kibana.yml
server.name: kibana
server.host: "0.0.0.0"
server.port: 5601
elasticsearch.hosts: ["http://elasticsearch:9200"]
elasticsearch.username: ""
elasticsearch.password: ""

# Configuration des dashboards par d√©faut
kibana.defaultAppId: "dashboard"
server.maxPayloadBytes: 1048576

# Param√®tres de s√©curit√©
server.xsrf.whitelist: ["/api/security/v1/login"]
```

### 1.4 Template Elasticsearch pour les logs

![Elasticsearch](https://img.shields.io/badge/Component-Elasticsearch-yellow)
![Mapping](https://img.shields.io/badge/Type-Index%20Mapping-green)

```json
{
  "index_patterns": ["security-logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 0,
      "index.refresh_interval": "5s"
    },
    "mappings": {
      "properties": {
        "@timestamp": {
          "type": "date"
        },
        "source_ip": {
          "type": "ip"
        },
        "dest_ip": {
          "type": "ip"
        },
        "response": {
          "type": "integer"
        },
        "bytes": {
          "type": "integer"
        },
        "threat_level": {
          "type": "keyword"
        },
        "severity": {
          "type": "keyword"
        },
        "tags": {
          "type": "keyword"
        },
        "geoip": {
          "properties": {
            "location": {
              "type": "geo_point"
            },
            "country_name": {
              "type": "keyword"
            },
            "city_name": {
              "type": "keyword"
            }
          }
        }
      }
    }
  }
}
```

---

## üìä PHASE 2: G√âN√âRATION DE DONN√âES D'EXEMPLE

![Phase](https://img.shields.io/badge/Phase-2-blue)
![Data Generation](https://img.shields.io/badge/Type-Data%20Generation-orange)
![Python](https://img.shields.io/badge/Language-Python-green)

### 2.1 G√©n√©rateur de logs Apache

![Apache](https://img.shields.io/badge/Log%20Type-Apache-red)
![Security](https://img.shields.io/badge/Focus-Security%20Events-yellow)

```python
#!/usr/bin/env python3
# scripts/generate_apache_logs.py

import random
import datetime
from faker import Faker
import json

class ApacheLogGenerator:
    def __init__(self):
        self.fake = Faker()
        self.suspicious_payloads = [
            "' OR '1'='1",
            "<script>alert('xss')</script>",
            "../../etc/passwd",
            "UNION SELECT * FROM users",
            "../../../windows/system32",
            "<img src=x onerror=alert(1)>",
            "'; DROP TABLE users; --",
            "%3Cscript%3Ealert%28%27XSS%27%29%3C/script%3E"
        ]
        
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36",
            "curl/7.68.0",
            "python-requests/2.25.1",
            "Nikto/2.1.6"
        ]
        
        self.normal_paths = [
            "/", "/index.html", "/about.html", "/contact.php",
            "/login.php", "/dashboard.php", "/api/users",
            "/images/logo.png", "/css/style.css", "/js/script.js"
        ]
        
        self.attack_paths = [
            "/admin/", "/phpmyadmin/", "/.env",
            "/wp-admin/", "/config.php", "/backup/"
        ]
    
    def generate_normal_log(self):
        """G√©n√©ration d'un log normal"""
        ip = self.fake.ipv4()
        timestamp = datetime.datetime.now().strftime("%d/%b/%Y:%H:%M:%S +0000")
        method = random.choice(["GET", "POST", "PUT"])
        path = random.choice(self.normal_paths)
        
        if random.random() < 0.1:  # 10% de chance d'avoir des param√®tres
            params = f"?id={random.randint(1, 100)}&page={random.randint(1, 10)}"
            path += params
        
        status_code = random.choices(
            [200, 301, 302, 404, 500],
            weights=[80, 5, 5, 8, 2]
        )[0]
        
        response_size = random.randint(500, 50000)
        user_agent = random.choice(self.user_agents[:3])  # Navigateurs normaux
        referer = random.choice(["-", "https://google.com", "https://example.com"])
        
        log_line = f'{ip} - - [{timestamp}] "{method} {path} HTTP/1.1" {status_code} {response_size} "{referer}" "{user_agent}"'
        return log_line
    
    def generate_attack_log(self):
        """G√©n√©ration d'un log d'attaque"""
        # IPs suspectes (souvent des VPN, Tor, etc.)
        suspicious_ips = [
            "185.220.100." + str(random.randint(1, 254)),
            "195.123.221." + str(random.randint(1, 254)),
            "77.247.181." + str(random.randint(1, 254))
        ]
        
        ip = random.choice(suspicious_ips)
        timestamp = datetime.datetime.now().strftime("%d/%b/%Y:%H:%M:%S +0000")
        method = random.choice(["GET", "POST"])
        
        # Type d'attaque
        attack_type = random.choice(["sqli", "xss", "path_traversal", "admin_access"])
        
        if attack_type == "sqli":
            payload = random.choice(self.suspicious_payloads[:4])
            path = f"/search.php?q={payload}"
        elif attack_type == "xss":
            payload = random.choice(self.suspicious_payloads[1:3])
            path = f"/comment.php?msg={payload}"
        elif attack_type == "path_traversal":
            payload = random.choice(self.suspicious_payloads[2:5])
            path = f"/download.php?file={payload}"
        else:  # admin_access
            path = random.choice(self.attack_paths)
        
        status_code = random.choices([200, 403, 404, 500], weights=[20, 40, 30, 10])[0]
        response_size = random.randint(100, 5000)
        user_agent = random.choice(self.user_agents[3:])  # Outils d'attaque
        
        log_line = f'{ip} - - [{timestamp}] "{method} {path} HTTP/1.1" {status_code} {response_size} "-" "{user_agent}"'
        return log_line
    
    def generate_logs(self, num_normal=1000, num_attacks=100, filename="logs/apache_access.log"):
        """G√©n√©ration d'un fichier de logs mixte"""
        logs = []
        
        # Logs normaux
        for _ in range(num_normal):
            logs.append(self.generate_normal_log())
        
        # Logs d'attaque
        for _ in range(num_attacks):
            logs.append(self.generate_attack_log())
        
        # M√©lange et tri par timestamp (approximatif)
        random.shuffle(logs)
        
        with open(filename, 'w') as f:
            for log in logs:
                f.write(log + '\n')
        
        print(f"[+] {len(logs)} logs g√©n√©r√©s dans {filename}")
        return logs

# Utilisation
if __name__ == "__main__":
    import os
    os.makedirs("logs", exist_ok=True)
    
    generator = ApacheLogGenerator()
    generator.generate_logs(num_normal=2000, num_attacks=200)
```

### 2.2 G√©n√©rateur de logs d'authentification

![Authentication](https://img.shields.io/badge/Log%20Type-Authentication-blue)
![Brute Force](https://img.shields.io/badge/Detection-Brute%20Force-red)

```python
#!/usr/bin/env python3
# scripts/generate_auth_logs.py

import random
import datetime
from faker import Faker

class AuthLogGenerator:
    def __init__(self):
        self.fake = Faker()
        self.usernames = [
            "admin", "root", "user", "test", "guest",
            "administrator", "service", "backup", "ftp"
        ]
        self.common_passwords = [
            "password", "123456", "admin", "qwerty",
            "letmein", "welcome", "monkey", "dragon"
        ]
        
        self.services = ["sshd", "sudo", "login", "su"]
        self.hostnames = ["server01", "web-server", "db-server"]
    
    def generate_failed_login(self):
        """G√©n√©ration d'une tentative de connexion √©chou√©e"""
        timestamp = datetime.datetime.now().strftime("%b %d %H:%M:%S")
        hostname = random.choice(self.hostnames)
        service = random.choice(self.services)
        pid = random.randint(1000, 9999)
        username = random.choice(self.usernames)
        source_ip = self.fake.ipv4()
        source_port = random.randint(1024, 65535)
        
        log_line = f"{timestamp} {hostname} {service}[{pid}]: Failed password for {username} from {source_ip} port {source_port} ssh2"
        return log_line
    
    def generate_successful_login(self):
        """G√©n√©ration d'une connexion r√©ussie"""
        timestamp = datetime.datetime.now().strftime("%b %d %H:%M:%S")
        hostname = random.choice(self.hostnames)
        service = random.choice(self.services)
        pid = random.randint(1000, 9999)
        username = random.choice(self.usernames[:3])  # Utilisateurs l√©gitimes
        source_ip = self.fake.ipv4()
        source_port = random.randint(1024, 65535)
        
        log_line = f"{timestamp} {hostname} {service}[{pid}]: Accepted password for {username} from {source_ip} port {source_port} ssh2"
        return log_line
    
    def generate_brute_force_sequence(self, target_user="admin", attempts=20):
        """G√©n√©ration d'une s√©quence de brute force"""
        attacker_ip = "192.168.1." + str(random.randint(100, 200))
        logs = []
        
        # Tentatives √©chou√©es
        for i in range(attempts - 1):
            timestamp = datetime.datetime.now().strftime("%b %d %H:%M:%S")
            hostname = random.choice(self.hostnames)
            service = "sshd"
            pid = random.randint(1000, 9999)
            source_port = random.randint(1024, 65535)
            
            log_line = f"{timestamp} {hostname} {service}[{pid}]: Failed password for {target_user} from {attacker_ip} port {source_port} ssh2"
            logs.append(log_line)
        
        # Derni√®re tentative r√©ussie (optionnel)
        if random.random() < 0.1:  # 10% de chance de r√©ussir
            timestamp = datetime.datetime.now().strftime("%b %d %H:%M:%S")
            hostname = random.choice(self.hostnames)
            service = "sshd"
            pid = random.randint(1000, 9999)
            source_port = random.randint(1024, 65535)
            
            log_line = f"{timestamp} {hostname} {service}[{pid}]: Accepted password for {target_user} from {attacker_ip} port {source_port} ssh2"
            logs.append(log_line)
        
        return logs
    
    def generate_logs(self, filename="logs/auth.log"):
        """G√©n√©ration du fichier de logs d'authentification"""
        logs = []
        
        # Logs normaux (connexions l√©gitimes)
        for _ in range(100):
            logs.append(self.generate_successful_login())
        
        # Tentatives √©chou√©es isol√©es
        for _ in range(50):
            logs.append(self.generate_failed_login())
        
        # S√©quences de brute force
        for _ in range(5):
            brute_force_logs = self.generate_brute_force_sequence()
            logs.extend(brute_force_logs)
        
        # Tri approximatif par timestamp
        random.shuffle(logs)
        
        with open(filename, 'w') as f:
            for log in logs:
                f.write(log + '\n')
        
        print(f"[+] {len(logs)} logs d'auth g√©n√©r√©s dans {filename}")
        return logs

# Utilisation
if __name__ == "__main__":
    import os
    os.makedirs("logs", exist_ok=True)
    
    generator = AuthLogGenerator()
    generator.generate_logs()
```

### 2.3 G√©n√©rateur de logs firewall

![Firewall](https://img.shields.io/badge/Log%20Type-Firewall-orange)
![Port Scan](https://img.shields.io/badge/Detection-Port%20Scan-red)

```python
#!/usr/bin/env python3
# scripts/generate_firewall_logs.py

import random
import datetime
from faker import Faker

class FirewallLogGenerator:
    def __init__(self):
        self.fake = Faker()
        self.protocols = ["TCP", "UDP", "ICMP"]
        self.actions = ["ALLOW", "DENY", "DROP"]
        
        # Ports communs
        self.common_ports = {
            "web": [80, 443, 8080, 8443],
            "ssh": [22, 2222],
            "database": [3306, 5432, 1433, 27017],
            "email": [25, 110, 143, 587, 993, 995],
            "dns": [53],
            "ftp": [21, 990],
            "remote": [3389, 5900, 23]
        }
        
        # IPs suspectes pour simulations d'attaques
        self.suspicious_ranges = [
            "185.220.100.", "195.123.221.", "77.247.181.",
            "192.42.116.", "171.25.193.", "198.96.155."
        ]
    
    def generate_normal_traffic(self):
        """G√©n√©ration de trafic r√©seau normal"""
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        action = "ALLOW"
        source_ip = self.fake.ipv4_private()
        dest_ip = self.fake.ipv4()
        protocol = random.choice(self.protocols)
        
        # Trafic web normal
        if random.random() < 0.6:
            source_port = random.randint(32768, 65535)
            dest_port = random.choice(self.common_ports["web"])
        # Trafic DNS
        elif random.random() < 0.3:
            source_port = random.randint(32768, 65535)
            dest_port = 53
            protocol = "UDP"
        # Autre trafic l√©gitime
        else:
            source_port = random.randint(1024, 65535)
            dest_port = random.choice([22, 443, 993, 587])
        
        log_line = f"{timestamp} {action} {source_ip}:{source_port} -> {dest_ip}:{dest_port} {protocol}"
        return log_line
    
    def generate_blocked_traffic(self):
        """G√©n√©ration de trafic bloqu√©/suspect"""
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        action = random.choice(["DENY", "DROP"])
        
        # IP suspecte
        source_ip = random.choice(self.suspicious_ranges) + str(random.randint(1, 254))
        dest_ip = self.fake.ipv4_private()
        protocol = random.choice(self.protocols)
        
        source_port = random.randint(1024, 65535)
        
        # Types d'attaques simul√©es
        attack_type = random.choice(["port_scan", "service_probe", "brute_force"])
        
        if attack_type == "port_scan":
            # Scan de ports s√©quentiel
            dest_port = random.choice(range(1, 1024))
        elif attack_type == "service_probe":
            # Tentative d'acc√®s aux services sensibles
            dest_port = random.choice([22, 23, 135, 445, 1433, 3389])
        else:  # brute_force
            # Tentatives multiples sur SSH/RDP
            dest_port = random.choice([22, 3389])
        
        log_line = f"{timestamp} {action} {source_ip}:{source_port} -> {dest_ip}:{dest_port} {protocol}"
        return log_line
    
    def generate_port_scan_sequence(self, scanner_ip=None):
        """G√©n√©ration d'une s√©quence de scan de ports"""
        if not scanner_ip:
            scanner_ip = random.choice(self.suspicious_ranges) + str(random.randint(1, 254))
        
        target_ip = self.fake.ipv4_private()
        logs = []
        
        # Scan de ports communs
        common_target_ports = [21, 22, 23, 25, 53, 80, 110, 135, 139, 443, 445, 993, 995, 1433, 3389, 5432]
        
        for port in random.sample(common_target_ports, random.randint(10, 20)):
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            action = "DENY"
            source_port = random.randint(32768, 65535)
            protocol = "TCP"
            
            log_line = f"{timestamp} {action} {scanner_ip}:{source_port} -> {target_ip}:{port} {protocol}"
            logs.append(log_line)
        
        return logs
    
    def generate_logs(self, filename="logs/firewall.log"):
        """G√©n√©ration du fichier de logs firewall"""
        logs = []
        
        # Trafic normal (majoritaire)
        for _ in range(800):
            logs.append(self.generate_normal_traffic())
        
        # Trafic bloqu√©/suspect
        for _ in range(150):
            logs.append(self.generate_blocked_traffic())
        
        # S√©quences de scan de ports
        for _ in range(3):
            scan_logs = self.generate_port_scan_sequence()
            logs.extend(scan_logs)
        
        # M√©lange des logs
        random.shuffle(logs)
        
        with open(filename, 'w') as f:
            for log in logs:
                f.write(log + '\n')
        
        print(f"[+] {len(logs)} logs firewall g√©n√©r√©s dans {filename}")
        return logs

# Utilisation
if __name__ == "__main__":
    import os
    os.makedirs("logs", exist_ok=True)
    
    generator = FirewallLogGenerator()
    generator.generate_logs()
```

---

## üîç PHASE 3: ANALYSE ET D√âTECTION D'INCIDENTS

![Phase](https://img.shields.io/badge/Phase-3-blue)
![Analysis](https://img.shields.io/badge/Type-Incident%20Analysis-red)
![Automation](https://img.shields.io/badge/Level-Automated-green)

### 3.1 Analyseur d'incidents automatis√©

![Python](https://img.shields.io/badge/Language-Python-green)
![Elasticsearch](https://img.shields.io/badge/Backend-Elasticsearch-yellow)
![Real Time](https://img.shields.io/badge/Processing-Real%20Time-orange)

```python
#!/usr/bin/env python3
# scripts/incident_analyzer.py

import json
import requests
from datetime import datetime, timedelta
from collections import Counter, defaultdict
import statistics

class IncidentAnalyzer:
    def __init__(self, elasticsearch_url="http://localhost:9200"):
        self.es_url = elasticsearch_url
        self.incidents = []
        
        # Seuils de d√©tection
        self.thresholds = {
            'failed_login_rate': 10,  # tentatives/minute
            'error_rate': 0.1,        # 10% d'erreurs
            'unique_ips_threshold': 50,
            'response_time_threshold': 5000  # ms
        }
    
    def query_elasticsearch(self, query, index="security-logs-*"):
        """Ex√©cution de requ√™tes Elasticsearch"""
        url = f"{self.es_url}/{index}/_search"
        headers = {'Content-Type': 'application/json'}
        
        try:
            response = requests.post(url, headers=headers, json=query)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            print(f"[-] Erreur Elasticsearch: {e}")
            return None
    
    def detect_brute_force_attacks(self):
        """D√©tection des attaques par brute force"""
        print("[+] Analyse des attaques par brute force...")
        
        # Requ√™te pour les tentatives de connexion √©chou√©es
        query = {
            "size": 0,
            "query": {
                "bool": {
                    "must": [
                        {"range": {"@timestamp": {"gte": "now-1h"}}},
                        {"terms": {"tags": ["failed_login", "brute_force_attempt"]}}
                    ]
                }
            },
            "aggs": {
                "by_source_ip": {
                    "terms": {
                        "field": "source_ip",
                        "size": 100
                    },
                    "aggs": {
                        "failed_attempts": {
                            "date_histogram": {
                                "field": "@timestamp",
                                "calendar_interval": "1m"
                            }
                        },
                        "targeted_users": {
                            "terms": {
                                "field": "failed_user.keyword",
                                "size": 10
                            }
                        }
                    }
                }
            }
        }
        
        result = self.query_elasticsearch(query)
        if not result:
            return []
        
        incidents = []
        for bucket in result['aggregations']['by_source_ip']['buckets']:
            source_ip = bucket['key']
            total_attempts = bucket['doc_count']
            
            # V√©rifier si le seuil est d√©pass√©
            if total_attempts > self.thresholds['failed_login_rate']:
                # Calculer la fr√©quence d'attaque
                time_buckets = bucket['failed_attempts']['buckets']
                max_attempts_per_minute = max([b['doc_count'] for b in time_buckets] or [0])
                
                # Utilisateurs cibl√©s
                targeted_users = [user['key'] for user in bucket['targeted_users']['buckets']]
                
                incident = {
                    'type': 'brute_force_attack',
                    'severity': 'high' if max_attempts_per_minute > 20 else 'medium',
                    'source_ip': source_ip,
                    'total_attempts': total_attempts,
                    'max_attempts_per_minute': max_attempts_per_minute,
                    'targeted_users': targeted_users,
                    'timestamp': datetime.now().isoformat(),
                    'description': f"Attaque par brute force d√©tect√©e depuis {source_ip} avec {total_attempts} tentatives"
                }
                incidents.append(incident)
        
        return incidents
    
    def detect_web_attacks(self):
        """D√©tection des attaques web (SQL injection, XSS, etc.)"""
        print("[+] Analyse des attaques web...")
        
        query = {
            "size": 100,
            "query": {
                "bool": {
                    "must": [
                        {"range": {"@timestamp": {"gte": "now-1h"}}},
                        {"terms": {"tags": ["suspicious", "potential_attack"]}}
                    ]
                }
            },
            "aggs": {
                "by_source_ip": {
                    "terms": {
                        "field": "source_ip",
                        "size": 50
                    }
                },
                "attack_types": {
                    "terms": {
                        "field": "request.keyword",
                        "size": 20
                    }
                }
            }
        }
        
        result = self.query_elasticsearch(query)
        if not result:
            return []
        
        incidents = []
        
        # Analyser par IP source
        for bucket in result['aggregations']['by_source_ip']['buckets']:
            source_ip = bucket['key']
            attack_count = bucket['doc_count']
            
            if attack_count > 5:  # Plus de 5 attaques par IP
                incident = {
                    'type': 'web_attack',
                    'severity': 'high' if attack_count > 20 else 'medium',
                    'source_ip': source_ip,
                    'attack_count': attack_count,
                    'timestamp': datetime.now().isoformat(),
                    'description': f"Attaque web d√©tect√©e depuis {source_ip} avec {attack_count} tentatives suspectes"
                }
                incidents.append(incident)
        
        # Analyser les types d'attaques sp√©cifiques
        for hit in result['hits']['hits']:
            source = hit['_source']
            if any(payload in source.get('request', '') for payload in ['union', 'select', 'script', 'alert']):
                incident = {
                    'type': 'specific_web_attack',
                    'severity': 'high',
                    'source_ip': source.get('source_ip'),
                    'request': source.get('request'),
                    'attack_pattern': self._identify_attack_pattern(source.get('request', '')),
                    'timestamp': source.get('@timestamp'),
                    'description': f"Attaque sp√©cifique d√©tect√©e: {self._identify_attack_pattern(source.get('request', ''))}"
                }
                incidents.append(incident)
        
        return incidents
    
    def detect_port_scans(self):
        """D√©tection des scans de ports"""
        print("[+] Analyse des scans de ports...")
        
        query = {
            "size": 0,
            "query": {
                "bool": {
                    "must": [
                        {"range": {"@timestamp": {"gte": "now-1h"}}},
                        {"terms": {"tags": ["port_scan", "reconnaissance"]}}
                    ]
                }
            },
            "aggs": {
                "by_source_ip": {
                    "terms": {
                        "field": "source_ip",
                        "size": 100
                    },
                    "aggs": {
                        "scanned_ports": {
                            "terms": {
                                "field": "dest_port",
                                "size": 100
                            }
                        },
                        "scan_timeline": {
                            "date_histogram": {
                                "field": "@timestamp",
                                "calendar_interval": "1m"
                            }
                        }
                    }
                }
            }
        }
        
        result = self.query_elasticsearch(query)
        if not result:
            return []
        
        incidents = []
        for bucket in result['aggregations']['by_source_ip']['buckets']:
            source_ip = bucket['key']
            total_scans = bucket['doc_count']
            scanned_ports = len(bucket['scanned_ports']['buckets'])
            
            # Seuil: plus de 10 ports scann√©s
            if scanned_ports > 10:
                # Analyser la rapidit√© du scan
                timeline = bucket['scan_timeline']['buckets']
                scan_duration = len([b for b in timeline if b['doc_count'] > 0])  # minutes actives
                
                ports_list = [str(port['key']) for port in bucket['scanned_ports']['buckets']]
                
                incident = {
                    'type': 'port_scan',
                    'severity': 'high' if scanned_ports > 50 else 'medium',
                    'source_ip': source_ip,
                    'scanned_ports_count': scanned_ports,
                    'total_attempts': total_scans,
                    'scan_duration_minutes': scan_duration,
                    'scanned_ports': ports_list[:20],  # Limiter la liste
                    'timestamp': datetime.now().isoformat(),
                    'description': f"Scan de ports d√©tect√© depuis {source_ip}: {scanned_ports} ports scann√©s"
                }
                incidents.append(incident)
        
        return incidents
    
    def detect_anomalies(self):
        """D√©tection d'anomalies g√©n√©rales"""
        print("[+] Analyse des anomalies...")
        
        # D√©tecter les pics de trafic
        traffic_query = {
            "size": 0,
            "query": {
                "range": {"@timestamp": {"gte": "now-1h"}}
            },
            "aggs": {
                "traffic_over_time": {
                    "date_histogram": {
                        "field": "@timestamp",
                        "calendar_interval": "5m"
                    }
                },
                "top_source_ips": {
                    "terms": {
                        "field": "source_ip",
                        "size": 20
                    }
                }
            }
        }
        
        result = self.query_elasticsearch(traffic_query)
        if not result:
            return []
        
        incidents = []
        
        # Analyser les pics de trafic
        traffic_buckets = result['aggregations']['traffic_over_time']['buckets']
        traffic_volumes = [bucket['doc_count'] for bucket in traffic_buckets]
        
        if traffic_volumes:
            avg_traffic = statistics.mean(traffic_volumes)
            max_traffic = max(traffic_volumes)
            
            # D√©tecter un pic anormal (3x la moyenne)
            if max_traffic > avg_traffic * 3 and max_traffic > 100:
                incident = {
                    'type': 'traffic_spike',
                    'severity': 'medium',
                    'max_requests': max_traffic,
                    'average_requests': round(avg_traffic, 2),
                    'spike_ratio': round(max_traffic / avg_traffic, 2),
                    'timestamp': datetime.now().isoformat(),
                    'description': f"Pic de trafic anormal d√©tect√©: {max_traffic} requ√™tes (moyenne: {avg_traffic:.2f})"
                }
                incidents.append(incident)
        
        # Analyser les IPs les plus actives
        for bucket in result['aggregations']['top_source_ips']['buckets']:
            ip = bucket['key']
            request_count = bucket['doc_count']
            
            # Seuil: plus de 1000 requ√™tes par heure
            if request_count > 1000:
                incident = {
                    'type': 'high_volume_source',
                    'severity': 'medium',
                    'source_ip': ip,
                    'request_count': request_count,
                    'timestamp': datetime.now().isoformat(),
                    'description': f"Volume anormalement √©lev√© depuis {ip}: {request_count} requ√™tes/heure"
                }
                incidents.append(incident)
        
        return incidents
    
    def _identify_attack_pattern(self, request):
        """Identification du type d'attaque √† partir de la requ√™te"""
        request_lower = request.lower()
        
        if any(pattern in request_lower for pattern in ['union', 'select', 'or 1=1', 'drop table']):
            return 'SQL Injection'
        elif any(pattern in request_lower for pattern in ['script', 'alert', 'javascript:', 'onerror']):
            return 'Cross-Site Scripting (XSS)'
        elif any(pattern in request_lower for pattern in ['../..', 'etc/passwd', 'windows/system32']):
            return 'Path Traversal'
        elif any(pattern in request_lower for pattern in ['cmd', 'exec', 'system']):
            return 'Command Injection'
        else:
            return 'Unknown Attack Pattern'
    
    def classify_incident_severity(self, incident):
        """Classification de la s√©v√©rit√© des incidents"""
        severity_score = 0
        
        # Facteurs de s√©v√©rit√©
        if incident['type'] in ['brute_force_attack', 'web_attack']:
            severity_score += 3
        elif incident['type'] in ['port_scan']:
            severity_score += 2
        else:
            severity_score += 1
        
        # Volume/fr√©quence
        if incident.get('total_attempts', 0) > 100:
            severity_score += 2
        elif incident.get('total_attempts', 0) > 50:
            severity_score += 1
        
        # Classification finale
        if severity_score >= 5:
            return 'critical'
        elif severity_score >= 3:
            return 'high'
        elif severity_score >= 2:
            return 'medium'
        else:
            return 'low'
    
    def generate_incident_report(self, incidents):
        """G√©n√©ration d'un rapport d'incidents"""
        if not incidents:
            return "Aucun incident d√©tect√©."
        
        # Tri par s√©v√©rit√© et timestamp
        incidents_sorted = sorted(incidents, 
                                key=lambda x: (x.get('severity', 'low'), x.get('timestamp', '')), 
                                reverse=True)
        
        report = f"""
![Incident Report](https://img.shields.io/badge/Report-Security%20Incidents-red)
![Count](https://img.shields.io/badge/Total%20Incidents-{len(incidents)}-orange)
![Status](https://img.shields.io/badge/Status-Active%20Monitoring-green)

# üö® RAPPORT D'INCIDENTS DE S√âCURIT√â
**P√©riode d'analyse**: Derni√®re heure  
**Nombre total d'incidents**: {len(incidents)}  
**G√©n√©r√© le**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## üìä R√©sum√© des incidents

"""
        
        # Comptage par type et s√©v√©rit√©
        incident_types = Counter([inc['type'] for inc in incidents])
        severity_counts = Counter([inc.get('severity', 'unknown') for inc in incidents])
        
        for inc_type, count in incident_types.most_common():
            report += f"- ![{inc_type}](https://img.shields.io/badge/{inc_type.replace('_', '%20')}-{count}-red) {inc_type.replace('_', ' ').title()}: {count} incidents\n"
        
        report += f"\n### üéØ R√©partition par s√©v√©rit√©\n"
        for severity, count in severity_counts.most_common():
            color = {'critical': 'red', 'high': 'orange', 'medium': 'yellow', 'low': 'green'}.get(severity, 'lightgrey')
            report += f"- ![{severity}](https://img.shields.io/badge/Severity-{severity.upper()}-{color}) {severity.upper()}: {count} incidents\n"
        
        report += f"\n## üìã D√©tail des incidents\n\n"
        
        for i, incident in enumerate(incidents_sorted[:10], 1):  # Top 10
            severity_color = {'critical': 'red', 'high': 'orange', 'medium': 'yellow', 'low': 'green'}.get(incident.get('severity', 'low'), 'lightgrey')
            
            report += f"""
### {i}. {incident['type'].replace('_', ' ').title()}
![Severity](https://img.shields.io/badge/Severity-{incident.get('severity', 'unknown').upper()}-{severity_color})
![Type](https://img.shields.io/badge/Type-{incident['type'].replace('_', '%20')}-blue)

**Description**: {incident['description']}  
**Timestamp**: `{incident.get('timestamp', 'N/A')}`  
**IP Source**: `{incident.get('source_ip', 'N/A')}`  
"""
            
            # D√©tails sp√©cifiques selon le type
            if incident['type'] == 'brute_force_attack':
                report += f"**Tentatives totales**: {incident.get('total_attempts', 'N/A')}  \n"
                report += f"**Utilisateurs cibl√©s**: {', '.join(incident.get('targeted_users', []))}  \n"
            elif incident['type'] == 'port_scan':
                report += f"**Ports scann√©s**: {incident.get('scanned_ports_count', 'N/A')}  \n"
                report += f"**Dur√©e du scan**: {incident.get('scan_duration_minutes', 'N/A')} minutes  \n"
            elif incident['type'] in ['web_attack', 'specific_web_attack']:
                report += f"**Pattern d'attaque**: {incident.get('attack_pattern', 'N/A')}  \n"
                if 'request' in incident:
                    report += f"**Requ√™te suspecte**: `{incident['request'][:100]}...`  \n"
        
        return report
    
    def run_analysis(self):
        """Ex√©cution de l'analyse compl√®te"""
        print(f"[+] D√©marrage de l'analyse des incidents - {datetime.now()}")
        
        all_incidents = []
        
        # Ex√©cuter toutes les d√©tections
        all_incidents.extend(self.detect_brute_force_attacks())
        all_incidents.extend(self.detect_web_attacks())
        all_incidents.extend(self.detect_port_scans())
        all_incidents.extend(self.detect_anomalies())
        
        # Classification de la s√©v√©rit√©
        for incident in all_incidents:
            if 'severity' not in incident:
                incident['severity'] = self.classify_incident_severity(incident)
        
        # G√©n√©ration du rapport
        report = self.generate_incident_report(all_incidents)
        
        # Sauvegarde du rapport
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_filename = f"reports/incident_report_{timestamp}.md"
        
        import os
        os.makedirs("reports", exist_ok=True)
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"[+] Analyse termin√©e. {len(all_incidents)} incidents d√©tect√©s.")
        print(f"[+] Rapport sauvegard√©: {report_filename}")
        
        return all_incidents, report

# Utilisation
if __name__ == "__main__":
    analyzer = IncidentAnalyzer()
    incidents, report = analyzer.run_analysis()
    print("\n" + "="*80)
    print(report)
```

### 3.2 Script de surveillance continue

![Monitoring](https://img.shields.io/badge/Type-Continuous%20Monitoring-green)
![Real Time](https://img.shields.io/badge/Mode-Real%20Time-orange)

```python
#!/usr/bin/env python3
# scripts/continuous_monitoring.py

import time
import schedule
import json
from datetime import datetime
from incident_analyzer import IncidentAnalyzer
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

class SecurityMonitor:
    def __init__(self, config_file="config/monitoring_config.json"):
        self.load_configuration(config_file)
        self.analyzer = IncidentAnalyzer(self.config.get('elasticsearch_url'))
        self.alert_history = []
    
    def load_configuration(self, config_file):
        """Chargement de la configuration"""
        try:
            with open(config_file, 'r') as f:
                self.config = json.load(f)
        except FileNotFoundError:
            print(f"[-] Fichier de configuration non trouv√©: {config_file}")
            self.config = self._default_config()
    
    def _default_config(self):
        """Configuration par d√©faut"""
        return {
            "elasticsearch_url": "http://localhost:9200",
            "monitoring_interval": 300,  # 5 minutes
            "alert_thresholds": {
                "critical_incidents": 1,
                "high_incidents": 3,
                "medium_incidents": 10
            },
            "notifications": {
                "email": {
                    "enabled": False,
                    "smtp_server": "smtp.gmail.com",
                    "smtp_port": 587,
                    "username": "",
                    "password": "",
                    "recipients": []
                },
                "webhook": {
                    "enabled": False,
                    "url": ""
                }
            }
        }
    
    def check_for_incidents(self):
        """V√©rification p√©riodique des incidents"""
        print(f"\n[+] V√©rification des incidents - {datetime.now().strftime('%H:%M:%S')}")
        
        try:
            incidents, report = self.analyzer.run_analysis()
            
            if incidents:
                self._process_incidents(incidents)
                return incidents
            else:
                print("[+] Aucun incident d√©tect√©")
                return []
        except Exception as e:
            print(f"[-] Erreur lors de l'analyse: {e}")
            return []
    
    def _process_incidents(self, incidents):
        """Traitement des incidents d√©tect√©s"""
        # Comptage par s√©v√©rit√©
        severity_counts = {
            'critical': len([i for i in incidents if i.get('severity') == 'critical']),
            'high': len([i for i in incidents if i.get('severity') == 'high']),
            'medium': len([i for i in incidents if i.get('severity') == 'medium']),
            'low': len([i for i in incidents if i.get('severity') == 'low'])
        }
        
        print(f"[!] {len(incidents)} incidents d√©tect√©s:")
        for severity, count in severity_counts.items():
            if count > 0:
                print(f"    - {severity.upper()}: {count}")
        
        # V√©rifier les seuils d'alerte
        alerts_triggered = []
        thresholds = self.config['alert_thresholds']
        
        if severity_counts['critical'] >= thresholds['critical_incidents']:
            alerts_triggered.append(f"CRITIQUE: {severity_counts['critical']} incidents critiques d√©tect√©s")
        
        if severity_counts['high'] >= thresholds['high_incidents']:
            alerts_triggered.append(f"IMPORTANT: {severity_counts['high']} incidents de haute s√©v√©rit√© d√©tect√©s")
        
        if severity_counts['medium'] >= thresholds['medium_incidents']:
            alerts_triggered.append(f"ATTENTION: {severity_counts['medium']} incidents de s√©v√©rit√© moyenne d√©tect√©s")
        
        # Envoyer les alertes si n√©cessaire
        if alerts_triggered:
            self._send_alerts(alerts_triggered, incidents)
    
    def _send_alerts(self, alerts, incidents):
        """Envoi des alertes"""
        alert_message = self._format_alert_message(alerts, incidents)
        
        # Email
        if self.config['notifications']['email']['enabled']:
            self._send_email_alert(alert_message)
        
        # Webhook
        if self.config['notifications']['webhook']['enabled']:
            self._send_webhook_alert(alert_message)
        
        # Console
        print("\nüö® ALERTE S√âCURIT√â üö®")
        print("=" * 50)
        print(alert_message)
        print("=" * 50)
    
    def _format_alert_message(self, alerts, incidents):
        """Formatage du message d'alerte"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        message = f"""
ALERTE S√âCURIT√â - {timestamp}

ALERTES D√âCLENCH√âES:
{chr(10).join(['- ' + alert for alert in alerts])}

INCIDENTS LES PLUS CRITIQUES:
"""
        
        # Afficher les 5 incidents les plus critiques
        critical_incidents = sorted(
            [i for i in incidents if i.get('severity') in ['critical', 'high']], 
            key=lambda x: x.get('severity', 'low'), 
            reverse=True
        )[:5]
        
        for incident in critical_incidents:
            message += f"\n- {incident['type'].upper()}: {incident['description']}"
            if 'source_ip' in incident:
                message += f" (IP: {incident['source_ip']})"
        
        return message
    
    def _send_email_alert(self, message):
        """Envoi d'alerte par email"""
        try:
            email_config = self.config['notifications']['email']
            
            msg = MIMEMultipart()
            msg['From'] = email_config['username']
            msg['To'] = ', '.join(email_config['recipients'])
            msg['Subject'] = f"üö® Alerte S√©curit√© SIEM - {datetime.now().strftime('%H:%M:%S')}"
            
            msg.attach(MIMEText(message, 'plain'))
            
            server = smtplib.SMTP(email_config['smtp_server'], email_config['smtp_port'])
            server.starttls()
            server.login(email_config['username'], email_config['password'])
            
            text = msg.as_string()
            server.sendmail(email_config['username'], email_config['recipients'], text)
            server.quit()
            
            print("[+] Alerte email envoy√©e")
        except Exception as e:
            print(f"[-] Erreur envoi email: {e}")
    
    def _send_webhook_alert(self, message):
        """Envoi d'alerte via webhook"""
        try:
            import requests
            
            webhook_url = self.config['notifications']['webhook']['url']
            payload = {
                'text': message,
                'timestamp': datetime.now().isoformat()
            }
            
            response = requests.post(webhook_url, json=payload)
            response.raise_for_status()
            
            print("[+] Alerte webhook envoy√©e")
        except Exception as e:
            print(f"[-] Erreur envoi webhook: {e}")
    
    def start_monitoring(self):
        """D√©marrage de la surveillance continue"""
        print(f"""
![Monitoring](https://img.shields.io/badge/Status-MONITORING%20STARTED-green)
![Interval](https://img.shields.io/badge/Interval-{self.config['monitoring_interval']}s-blue)

üõ°Ô∏è D√âMARRAGE DE LA SURVEILLANCE S√âCURIT√â
        
Configuration:
- URL Elasticsearch: {self.config['elasticsearch_url']}
- Intervalle de v√©rification: {self.config['monitoring_interval']} secondes
- Seuils d'alerte: {self.config['alert_thresholds']}
- Notifications email: {'‚úÖ' if self.config['notifications']['email']['enabled'] else '‚ùå'}
- Notifications webhook: {'‚úÖ' if self.config['notifications']['webhook']['enabled'] else '‚ùå'}

Surveillance active... Appuyez sur Ctrl+C pour arr√™ter.
        """)
        
        # Configuration du planning
        interval_minutes = self.config['monitoring_interval'] // 60
        if interval_minutes > 0:
            schedule.every(interval_minutes).minutes.do(self.check_for_incidents)
        else:
            schedule.every(self.config['monitoring_interval']).seconds.do(self.check_for_incidents)
        
        # Premi√®re v√©rification imm√©diate
        self.check_for_incidents()
        
        # Boucle principale de surveillance
        try:
            while True:
                schedule.run_pending()
                time.sleep(1)
        except KeyboardInterrupt:
            print("\n[+] Surveillance arr√™t√©e par l'utilisateur")

# Utilisation
if __name__ == "__main__":
    monitor = SecurityMonitor()
    monitor.start_monitoring()
```

---

## üìÅ STRUCTURE DU PROJET

![Project Structure](https://img.shields.io/badge/Project-Structure-blue)
![Organization](https://img.shields.io/badge/Type-Organized-green)

```
TASK-02-SIEM-Operations/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ README.md
‚îÇ   ![Documentation](https://img.shields.io/badge/Type-Documentation-blue)
‚îÇ
‚îú‚îÄ‚îÄ üê≥ docker/
‚îÇ   ‚îÇ   ![Docker](https://img.shields.io/badge/Container-Docker-blue)
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml
‚îÇ       ![Compose](https://img.shields.io/badge/Type-Compose%20File-orange)
‚îÇ
‚îú‚îÄ‚îÄ ‚öôÔ∏è config/
‚îÇ   ‚îÇ   ![Configuration](https://img.shields.io/badge/Type-Configuration-yellow)
‚îÇ   ‚îú‚îÄ‚îÄ logstash.conf
‚îÇ   ‚îÇ   ![Logstash](https://img.shields.io/badge/Service-Logstash-green)
‚îÇ   ‚îú‚îÄ‚îÄ elasticsearch.yml
‚îÇ   ‚îÇ   ![Elasticsearch](https://img.shields.io/badge/Service-Elasticsearch-yellow)
‚îÇ   ‚îú‚îÄ‚îÄ kibana.yml
‚îÇ   ‚îÇ   ![Kibana](https://img.shields.io/badge/Service-Kibana-purple)
‚îÇ   ‚îú‚îÄ‚îÄ filebeat.yml
‚îÇ   ‚îÇ   ![Filebeat](https://img.shields.io/badge/Service-Filebeat-red)
‚îÇ   ‚îî‚îÄ‚îÄ monitoring_config.json
‚îÇ       ![Monitoring](https://img.shields.io/badge/Type-Monitoring%20Config-orange)
‚îÇ
‚îú‚îÄ‚îÄ üêç scripts/
‚îÇ   ‚îÇ   ![Python Scripts](https://img.shields.io/badge/Language-Python-green)
‚îÇ   ‚îú‚îÄ‚îÄ generate_apache_logs.py
‚îÇ   ‚îÇ   ![Apache Logs](https://img.shields.io/badge/Generator-Apache%20Logs-red)
‚îÇ   ‚îú‚îÄ‚îÄ generate_auth_logs.py
‚îÇ   ‚îÇ   ![Auth Logs](https://img.shields.io/badge/Generator-Auth%20Logs-blue)
‚îÇ   ‚îú‚îÄ‚îÄ generate_firewall_logs.py
‚îÇ   ‚îÇ   ![Firewall Logs](https://img.shields.io/badge/Generator-Firewall%20Logs-orange)
‚îÇ   ‚îú‚îÄ‚îÄ incident_analyzer.py
‚îÇ   ‚îÇ   ![Analysis](https://img.shields.io/badge/Tool-Incident%20Analyzer-red)
‚îÇ   ‚îú‚îÄ‚îÄ incident_classifier.py
‚îÇ   ‚îÇ   ![Classification](https://img.shields.io/badge/Tool-Incident%20Classifier-yellow)
‚îÇ   ‚îú‚îÄ‚îÄ alert_correlation.py
‚îÇ   ‚îÇ   ![Correlation](https://img.shields.io/badge/Tool-Alert%20Correlation-purple)
‚îÇ   ‚îú‚îÄ‚îÄ continuous_monitoring.py
‚îÇ   ‚îÇ   ![Monitoring](https://img.shields.io/badge/Tool-Continuous%20Monitor-green)
‚îÇ   ‚îî‚îÄ‚îÄ response_playbook.py
‚îÇ       ![Playbook](https://img.shields.io/badge/Tool-Response%20Playbook-blue)
‚îÇ
‚îú‚îÄ‚îÄ üìä reports/
‚îÇ   ‚îÇ   ![Reports](https://img.shields.io/badge/Type-Security%20Reports-red)
‚îÇ   ‚îú‚îÄ‚îÄ incident_classification_report.json
‚îÇ   ‚îÇ   ![Classification](https://img.shields.io/badge/Report-Classification-yellow)
‚îÇ   ‚îú‚îÄ‚îÄ alert_correlation_report.json
‚îÇ   ‚îÇ   ![Correlation](https://img.shields.io/badge/Report-Correlation-purple)
‚îÇ   ‚îú‚îÄ‚îÄ incident_response_report.md
‚îÇ   ‚îÇ   ![Response](https://img.shields.io/badge/Report-Response-blue)
‚îÇ   ‚îî‚îÄ‚îÄ daily_security_summary.md
‚îÇ       ![Summary](https://img.shields.io/badge/Report-Daily%20Summary-green)
‚îÇ
‚îú‚îÄ‚îÄ üìà dashboards/
‚îÇ   ‚îÇ   ![Dashboards](https://img.shields.io/badge/Type-Visualizations-purple)
‚îÇ   ‚îú‚îÄ‚îÄ soc_dashboard.png
‚îÇ   ‚îÇ   ![SOC](https://img.shields.io/badge/Dashboard-SOC%20Operations-blue)
‚îÇ   ‚îú‚îÄ‚îÄ threat_intelligence.png
‚îÇ   ‚îÇ   ![Threat Intel](https://img.shields.io/badge/Dashboard-Threat%20Intelligence-red)
‚îÇ   ‚îú‚îÄ‚îÄ incident_timeline.png
‚îÇ   ‚îÇ   ![Timeline](https://img.shields.io/badge/Dashboard-Incident%20Timeline-orange)
‚îÇ   ‚îî‚îÄ‚îÄ network_security_overview.png
‚îÇ       ![Network](https://img.shields.io/badge/Dashboard-Network%20Security-green)
‚îÇ
‚îú‚îÄ‚îÄ üìù logs/
‚îÇ   ‚îÇ   ![Log Files](https://img.shields.io/badge/Type-Log%20Storage-grey)
‚îÇ   ‚îú‚îÄ‚îÄ apache_access.log
‚îÇ   ‚îú‚îÄ‚îÄ auth.log
‚îÇ   ‚îú‚îÄ‚îÄ firewall.log
‚îÇ   ‚îî‚îÄ‚îÄ application.log
‚îÇ
‚îú‚îÄ‚îÄ üé≠ templates/
‚îÇ   ‚îÇ   ![Templates](https://img.shields.io/badge/Type-Templates-lightblue)
‚îÇ   ‚îú‚îÄ‚îÄ incident_response_template.md
‚îÇ   ‚îú‚îÄ‚îÄ security_alert_template.json
‚îÇ   ‚îî‚îÄ‚îÄ playbook_template.py
‚îÇ
‚îî‚îÄ‚îÄ üìö documentation/
    ‚îÇ   ![Documentation](https://img.shields.io/badge/Type-Documentation-blue)
    ‚îú‚îÄ‚îÄ setup_guide.md
    ‚îú‚îÄ‚îÄ playbook_procedures.md
    ‚îú‚îÄ‚îÄ troubleshooting.md
    ‚îî‚îÄ‚îÄ best_practices.md
```

---

## üîß PHASE 4: OUTILS COMPL√âMENTAIRES

![Phase](https://img.shields.io/badge/Phase-4-blue)
![Tools](https://img.shields.io/badge/Type-Additional%20Tools-green)

### 4.1 Classificateur d'incidents avanc√©

![Classification](https://img.shields.io/badge/Tool-Incident%20Classifier-yellow)
![Machine Learning](https://img.shields.io/badge/Technology-ML%20Enhanced-purple)

```python
#!/usr/bin/env python3
# scripts/incident_classifier.py

import json
import numpy as np
from datetime import datetime, timedelta
from collections import defaultdict
import requests
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import joblib

class AdvancedIncidentClassifier:
    def __init__(self, elasticsearch_url="http://localhost:9200"):
        self.es_url = elasticsearch_url
        self.classification_rules = self._load_classification_rules()
        self.ml_model = None
        self.vectorizer = None
        
    def _load_classification_rules(self):
        """Chargement des r√®gles de classification"""
        return {
            # Attaques par force brute
            'brute_force': {
                'indicators': [
                    'failed_login', 'brute_force_attempt', 'authentication_failure'
                ],
                'thresholds': {
                    'attempts_per_minute': 10,
                    'unique_users_targeted': 3,
                    'source_ip_reputation': 0.3
                },
                'severity_matrix': {
                    'low': {'attempts': (1, 10), 'duration': (1, 300)},
                    'medium': {'attempts': (11, 50), 'duration': (301, 1800)},
                    'high': {'attempts': (51, 200), 'duration': (1801, 3600)},
                    'critical': {'attempts': (201, float('inf')), 'duration': (3601, float('inf'))}
                }
            },
            
            # Attaques web
            'web_attack': {
                'indicators': [
                    'sql_injection', 'xss', 'path_traversal', 'command_injection'
                ],
                'patterns': {
                    'sql_injection': [r'union\s+select', r'or\s+1\s*=\s*1', r'drop\s+table'],
                    'xss': [r'<script', r'javascript:', r'onerror\s*='],
                    'path_traversal': [r'\.\./', r'etc/passwd', r'windows/system32'],
                    'command_injection': [r';.*cmd', r'\|.*ls', r'`.*whoami']
                },
                'severity_matrix': {
                    'low': {'successful_attempts': 0, 'payload_complexity': 1},
                    'medium': {'successful_attempts': (1, 5), 'payload_complexity': 2},
                    'high': {'successful_attempts': (6, 20), 'payload_complexity': 3},
                    'critical': {'successful_attempts': (21, float('inf')), 'payload_complexity': 4}
                }
            },
            
            # Reconnaissance et scanning
            'reconnaissance': {
                'indicators': [
                    'port_scan', 'service_enumeration', 'vulnerability_scan'
                ],
                'thresholds': {
                    'ports_scanned': 20,
                    'scan_speed': 10,  # ports per minute
                    'scan_comprehensiveness': 0.5
                },
                'severity_matrix': {
                    'low': {'ports': (1, 10), 'duration': (60, 300)},
                    'medium': {'ports': (11, 50), 'duration': (301, 900)},
                    'high': {'ports': (51, 200), 'duration': (901, 1800)},
                    'critical': {'ports': (201, float('inf')), 'duration': (1801, float('inf'))}
                }
            },
            
            # Malware et intrusions
            'malware': {
                'indicators': [
                    'malicious_payload', 'suspicious_file_upload', 'backdoor_communication'
                ],
                'file_signatures': [
                    'executable_upload', 'suspicious_extension', 'encoded_payload'
                ],
                'severity_matrix': {
                    'medium': {'confirmed_malware': False, 'suspicious_behavior': True},
                    'high': {'confirmed_malware': True, 'data_exfiltration': False},
                    'critical': {'confirmed_malware': True, 'data_exfiltration': True}
                }
            }
        }
    
    def classify_incident(self, incident_data):
        """Classification d'un incident"""
        print(f"[+] Classification de l'incident: {incident_data.get('type', 'Unknown')}")
        
        # Extraction des caract√©ristiques
        features = self._extract_features(incident_data)
        
        # Classification principale
        primary_classification = self._classify_primary_type(incident_data, features)
        
        # √âvaluation de la s√©v√©rit√©
        severity = self._evaluate_severity(incident_data, features, primary_classification)
        
        # Calcul du score de risque
        risk_score = self._calculate_risk_score(incident_data, features, severity)
        
        # Attribution des priorit√©s
        priority = self._assign_priority(severity, risk_score, features)
        
        classification_result = {
            'incident_id': incident_data.get('id', self._generate_incident_id()),
            'timestamp': datetime.now().isoformat(),
            'primary_classification': primary_classification,
            'sub_classifications': self._identify_sub_classifications(incident_data, features),
            'severity': severity,
            'risk_score': risk_score,
            'priority': priority,
            'confidence': self._calculate_confidence(incident_data, features),
            'features': features,
            'recommended_actions': self._suggest_actions(primary_classification, severity),
            'escalation_required': self._requires_escalation(severity, risk_score),
            'estimated_impact': self._estimate_impact(incident_data, features, severity)
        }
        
        return classification_result
    
    def _extract_features(self, incident_data):
        """Extraction des caract√©ristiques de l'incident"""
        features = {
            'temporal': self._extract_temporal_features(incident_data),
            'network': self._extract_network_features(incident_data),
            'behavioral': self._extract_behavioral_features(incident_data),
            'contextual': self._extract_contextual_features(incident_data)
        }
        return features
    
    def _extract_temporal_features(self, incident_data):
        """Extraction des caract√©ristiques temporelles"""
        return {
            'duration': incident_data.get('duration', 0),
            'frequency': incident_data.get('frequency', 1),
            'time_of_day': datetime.now().hour,
            'day_of_week': datetime.now().weekday(),
            'is_business_hours': 9 <= datetime.now().hour <= 17,
            'temporal_pattern': self._analyze_temporal_pattern(incident_data)
        }
    
    def _extract_network_features(self, incident_data):
        """Extraction des caract√©ristiques r√©seau"""
        source_ip = incident_data.get('source_ip', '')
        return {
            'source_ip_reputation': self._check_ip_reputation(source_ip),
            'geographic_location': incident_data.get('geoip', {}),
            'is_internal_ip': self._is_internal_ip(source_ip),
            'port_diversity': len(set(incident_data.get('ports_involved', []))),
            'protocol_types': list(set(incident_data.get('protocols', []))),
            'bandwidth_usage': incident_data.get('bandwidth', 0)
        }
    
    def _extract_behavioral_features(self, incident_data):
        """Extraction des caract√©ristiques comportementales"""
        return {
            'attack_sophistication': self._assess_sophistication(incident_data),
            'evasion_techniques': self._detect_evasion_techniques(incident_data),
            'persistence_indicators': self._check_persistence(incident_data),
            'lateral_movement': self._detect_lateral_movement(incident_data),
            'data_exfiltration_signs': self._check_data_exfiltration(incident_data)
        }
    
    def _extract_contextual_features(self, incident_data):
        """Extraction des caract√©ristiques contextuelles"""
        return {
            'affected_systems': incident_data.get('affected_systems', []),
            'business_criticality': self._assess_business_criticality(incident_data),
            'user_privileges': incident_data.get('user_privileges', 'unknown'),
            'data_sensitivity': self._assess_data_sensitivity(incident_data),
            'regulatory_implications': self._check_regulatory_implications(incident_data)
        }
    
    def _classify_primary_type(self, incident_data, features):
        """Classification du type principal d'incident"""
        incident_type = incident_data.get('type', '')
        tags = incident_data.get('tags', [])
        
        # Mapping des types
        type_mapping = {
            'brute_force_attack': 'authentication_attack',
            'web_attack': 'application_attack',
            'port_scan': 'reconnaissance',
            'malware_detection': 'malware_incident',
            'data_exfiltration': 'data_breach',
            'privilege_escalation': 'privilege_abuse',
            'lateral_movement': 'advanced_persistent_threat'
        }
        
        primary_type = type_mapping.get(incident_type, 'unknown_incident')
        
        # Affinement bas√© sur les tags et caract√©ristiques
        if 'failed_login' in tags and features['behavioral']['attack_sophistication'] > 0.7:
            primary_type = 'advanced_authentication_attack'
        elif 'sql_injection' in tags or 'xss' in tags:
            primary_type = 'web_application_attack'
        elif features['behavioral']['lateral_movement'] and features['behavioral']['persistence_indicators']:
            primary_type = 'advanced_persistent_threat'
        
        return primary_type
    
    def _evaluate_severity(self, incident_data, features, classification):
        """√âvaluation de la s√©v√©rit√©"""
        base_severity = incident_data.get('severity', 'medium')
        
        # Facteurs d'aggravation
        aggravating_factors = 0
        
        # Criticit√© business
        if features['contextual']['business_criticality'] == 'critical':
            aggravating_factors += 2
        elif features['contextual']['business_criticality'] == 'high':
            aggravating_factors += 1
        
        # Sensibilit√© des donn√©es
        if features['contextual']['data_sensitivity'] == 'confidential':
            aggravating_factors += 2
        elif features['contextual']['data_sensitivity'] == 'sensitive':
            aggravating_factors += 1
        
        # Sophistication de l'attaque
        if features['behavioral']['attack_sophistication'] > 0.8:
            aggravating_factors += 2
        elif features['behavioral']['attack_sophistication'] > 0.6:
            aggravating_factors += 1
        
        # Persistance et mouvement lat√©ral
        if features['behavioral']['persistence_indicators']:
            aggravating_factors += 1
        if features['behavioral']['lateral_movement']:
            aggravating_factors += 2
        
        # Calcul de la s√©v√©rit√© finale
        severity_score = self._severity_to_score(base_severity) + aggravating_factors
        
        if severity_score >= 8:
            return 'critical'
        elif severity_score >= 6:
            return 'high'
        elif severity_score >= 4:
            return 'medium'
        else:
            return 'low'
    
    def _calculate_risk_score(self, incident_data, features, severity):
        """Calcul du score de risque (0-100)"""
        # Score de base selon la s√©v√©rit√©
        severity_scores = {'low': 20, 'medium': 40, 'high': 70, 'critical': 90}
        base_score = severity_scores.get(severity, 40)
        
        # Facteurs de risque additionnels
        risk_factors = {
            'ip_reputation': (1 - features['network']['source_ip_reputation']) * 10,
            'attack_sophistication': features['behavioral']['attack_sophistication'] * 15,
            'business_impact': self._calculate_business_impact(features) * 20,
            'data_sensitivity': self._calculate_data_sensitivity_score(features) * 15,
            'evasion_techniques': len(features['behavioral']['evasion_techniques']) * 5
        }
        
        total_risk = base_score + sum(risk_factors.values())
        return min(100, max(0, total_risk))  # Normalisation 0-100
    
    def _assign_priority(self, severity, risk_score, features):
        """Attribution de la priorit√©"""
        # Matrice priorit√© bas√©e sur s√©v√©rit√© et score de risque
        if severity == 'critical' or risk_score >= 85:
            return 'P1 - Critical'
        elif severity == 'high' or risk_score >= 70:
            return 'P2 - High'
        elif severity == 'medium' or risk_score >= 40:
            return 'P3 - Medium'
        else:
            return 'P4 - Low'
    
    def _suggest_actions(self, classification, severity):
        """Suggestion d'actions √† prendre"""
        action_matrix = {
            'authentication_attack': {
                'immediate': ['Block source IP', 'Reset affected user passwords', 'Enable MFA'],
                'short_term': ['Review authentication logs', 'Update password policies'],
                'long_term': ['Implement account lockout policies', 'Deploy behavioral analytics']
            },
            'web_application_attack': {
                'immediate': ['Block malicious requests', 'Apply WAF rules', 'Patch vulnerabilities'],
                'short_term': ['Code review', 'Security testing', 'Update input validation'],
                'long_term': ['SAST/DAST implementation', 'Security training for developers']
            },
            'reconnaissance': {
                'immediate': ['Block scanning IP', 'Alert network team', 'Increase monitoring'],
                'short_term': ['Review firewall rules', 'Analyze scan patterns'],
                'long_term': ['Deploy deception technology', 'Network segmentation']
            }
        }
        
        return action_matrix.get(classification, {
            'immediate': ['Isolate affected systems', 'Collect evidence'],
            'short_term': ['Detailed investigation', 'Impact assessment'],
            'long_term': ['Security improvements', 'Process review']
        })
    
    def generate_classification_report(self, incidents):
        """G√©n√©ration d'un rapport de classification"""
        if not incidents:
            return "Aucun incident √† classifier."
        
        # Classification de tous les incidents
        classified_incidents = []
        for incident in incidents:
            classification = self.classify_incident(incident)
            classified_incidents.append(classification)
        
        # Analyse statistique
        severity_distribution = defaultdict(int)
        risk_scores = []
        classifications = defaultdict(int)
        
        for incident in classified_incidents:
            severity_distribution[incident['severity']] += 1
            risk_scores.append(incident['risk_score'])
            classifications[incident['primary_classification']] += 1
        
        avg_risk_score = np.mean(risk_scores) if risk_scores else 0
        
        # G√©n√©ration du rapport
        report = f"""
![Classification Report](https://img.shields.io/badge/Report-Incident%20Classification-purple)
![Total](https://img.shields.io/badge/Total%20Incidents-{len(classified_incidents)}-blue)
![Avg Risk](https://img.shields.io/badge/Avg%20Risk%20Score-{avg_risk_score:.1f}-orange)

# üéØ RAPPORT DE CLASSIFICATION DES INCIDENTS

**P√©riode d'analyse**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Nombre d'incidents classifi√©s**: {len(classified_incidents)}  
**Score de risque moyen**: {avg_risk_score:.2f}/100

## üìä Distribution par s√©v√©rit√©

"""
        
        for severity in ['critical', 'high', 'medium', 'low']:
            count = severity_distribution[severity]
            if count > 0:
                percentage = (count / len(classified_incidents)) * 100
                color = {'critical': 'red', 'high': 'orange', 'medium': 'yellow', 'low': 'green'}[severity]
                report += f"- ![{severity}](https://img.shields.io/badge/{severity.upper()}-{count}%20({percentage:.1f}%%)-{color})\n"
        
        report += f"\n## üéØ Types d'incidents d√©tect√©s\n\n"
        
        for classification, count in classifications.most_common():
            percentage = (count / len(classified_incidents)) * 100
            report += f"- **{classification.replace('_', ' ').title()}**: {count} incidents ({percentage:.1f}%)\n"
        
        report += f"\n## üö® Incidents prioritaires\n\n"
        
        # Top 10 des incidents les plus critiques
        priority_incidents = sorted(classified_incidents, 
                                  key=lambda x: x['risk_score'], reverse=True)[:10]
        
        for i, incident in enumerate(priority_incidents, 1):
            severity_color = {'critical': 'red', 'high': 'orange', 'medium': 'yellow', 'low': 'green'}.get(incident['severity'], 'grey')
            report += f"""
### {i}. {incident['primary_classification'].replace('_', ' ').title()}
![Priority](https://img.shields.io/badge/Priority-{incident['priority'].replace(' ', '%20')}-red)
![Risk](https://img.shields.io/badge/Risk%20Score-{incident['risk_score']:.0f}/100-orange)
![Severity](https://img.shields.io/badge/Severity-{incident['severity'].upper()}-{severity_color})

**Impact estim√©**: {incident['estimated_impact']}  
**Escalade requise**: {'‚úÖ Oui' if incident['escalation_required'] else '‚ùå Non'}  
**Confiance**: {incident['confidence']:.1%}

**Actions recommand√©es**:
- Imm√©diat: {', '.join(incident['recommended_actions']['immediate'][:2])}
- Court terme: {', '.join(incident['recommended_actions']['short_term'][:2])}

---
"""
        
        # Sauvegarde du rapport
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_filename = f"reports/classification_report_{timestamp}.md"
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # Sauvegarde des donn√©es JSON pour analyse ult√©rieure
        json_filename = f"reports/classification_data_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(classified_incidents, f, indent=2, default=str)
        
        print(f"[+] Rapport de classification g√©n√©r√©: {report_filename}")
        print(f"[+] Donn√©es JSON sauvegard√©es: {json_filename}")
        
        return report, classified_incidents
    
    # M√©thodes utilitaires
    def _severity_to_score(self, severity):
        return {'low': 1, 'medium': 3, 'high': 5, 'critical': 7}.get(severity, 3)
    
    def _generate_incident_id(self):
        return f"INC-{datetime.now().strftime('%Y%m%d%H%M%S')}-{np.random.randint(1000, 9999)}"
    
    def _check_ip_reputation(self, ip):
        # Simulation - dans un vrai environnement, interroger des services comme VirusTotal
        suspicious_ranges = ['185.220.', '77.247.', '195.123.']
        return 0.2 if any(ip.startswith(range_) for range_ in suspicious_ranges) else 0.8
    
    def _is_internal_ip(self, ip):
        import ipaddress
        try:
            ip_obj = ipaddress.ip_address(ip)
            return ip_obj.is_private
        except:
            return False
    
    def _assess_sophistication(self, incident_data):
        # √âvaluation bas√©e sur la complexit√© des patterns d'attaque
        sophistication_indicators = [
            'encrypted_payload', 'polymorphic_code', 'zero_day_exploit',
            'advanced_evasion', 'multi_stage_attack'
        ]
        tags = incident_data.get('tags', [])
        score = sum(1 for indicator in sophistication_indicators if indicator in tags)
        return min(1.0, score / len(sophistication_indicators))
    
    def _detect_evasion_techniques(self, incident_data):
        evasion_indicators = []
        request = incident_data.get('request', '').lower()
        
        if '%' in request:  # URL encoding
            evasion_indicators.append('url_encoding')
        if 'char(' in request:  # SQL character functions
            evasion_indicators.append('sql_char_functions')
        if len(request) > 1000:  # Oversized requests
            evasion_indicators.append('oversized_request')
        
        return evasion_indicators
    
    def _check_persistence(self, incident_data):
        persistence_tags = ['scheduled_task', 'registry_modification', 'service_creation', 'startup_modification']
        return any(tag in incident_data.get('tags', []) for tag in persistence_tags)
    
    def _detect_lateral_movement(self, incident_data):
        movement_indicators = ['smb_enumeration', 'rdp_connections', 'credential_dumping', 'network_discovery']
        return any(indicator in incident_data.get('tags', []) for indicator in movement_indicators)
    
    def _check_data_exfiltration(self, incident_data):
        exfiltration_tags = ['large_data_transfer', 'dns_tunneling', 'encrypted_channel', 'unusual_upload']
        return any(tag in incident_data.get('tags', []) for tag in exfiltration_tags)
    
    def _assess_business_criticality(self, incident_data):
        affected_systems = incident_data.get('affected_systems', [])
        critical_systems = ['database_server', 'domain_controller', 'payment_system', 'customer_portal']
        
        if any(system in critical_systems for system in affected_systems):
            return 'critical'
        elif 'production' in str(affected_systems).lower():
            return 'high'
        else:
            return 'medium'
    
    def _assess_data_sensitivity(self, incident_data):
        data_types = incident_data.get('data_types', [])
        if any(dt in ['pii', 'financial', 'medical'] for dt in data_types):
            return 'confidential'
        elif any(dt in ['internal', 'proprietary'] for dt in data_types):
            return 'sensitive'
        else:
            return 'public'
    
    def _check_regulatory_implications(self, incident_data):
        regulations = []
        data_types = incident_data.get('data_types', [])
        
        if 'pii' in data_types:
            regulations.extend(['GDPR', 'CCPA'])
        if 'financial' in data_types:
            regulations.append('PCI-DSS')
        if 'medical' in data_types:
            regulations.append('HIPAA')
        
        return regulations
    
    def _calculate_business_impact(self, features):
        criticality_scores = {'critical': 1.0, 'high': 0.7, 'medium': 0.4, 'low': 0.1}
        return criticality_scores.get(features['contextual']['business_criticality'], 0.4)
    
    def _calculate_data_sensitivity_score(self, features):
        sensitivity_scores = {'confidential': 1.0, 'sensitive': 0.6, 'public': 0.2}
        return sensitivity_scores.get(features['contextual']['data_sensitivity'], 0.4)
    
    def _calculate_confidence(self, incident_data, features):
        confidence_factors = []
        
        # Qualit√© des donn√©es
        if incident_data.get('source_ip'):
            confidence_factors.append(0.2)
        if incident_data.get('timestamp'):
            confidence_factors.append(0.2)
        if incident_data.get('tags'):
            confidence_factors.append(0.3)
        
        # Corr√©lation avec patterns connus
        if features['behavioral']['attack_sophistication'] > 0.5:
            confidence_factors.append(0.3)
        
        return sum(confidence_factors)
    
    def _requires_escalation(self, severity, risk_score):
        """D√©termine si l'incident n√©cessite une escalade"""
        return severity in ['critical', 'high'] or risk_score >= 75
    
    def _estimate_impact(self, incident_data, features, severity):
        """Estimation de l'impact de l'incident"""
        impact_levels = {
            'critical': 'Impact majeur sur les op√©rations business critiques',
            'high': 'Impact significatif sur les services ou donn√©es sensibles',
            'medium': 'Impact mod√©r√© sur les op√©rations normales',
            'low': 'Impact minimal sur les syst√®mes non-critiques'
        }
        return impact_levels.get(severity, 'Impact √† d√©terminer')
    
    def _analyze_temporal_pattern(self, incident_data):
        """Analyse des patterns temporels"""
        # Dans un environnement r√©el, analyser les tendances historiques
        return 'normal'  # Placeholder

# Utilisation
if __name__ == "__main__":
    classifier = AdvancedIncidentClassifier()
    
    # Exemple d'incident √† classifier
    sample_incident = {
        'type': 'brute_force_attack',
        'source_ip': '185.220.100.45',
        'total_attempts': 150,
        'targeted_users': ['admin', 'root'],
        'tags': ['failed_login', 'brute_force_attempt'],
        'affected_systems': ['web_server', 'database_server'],
        'duration': 1800,  # 30 minutes
        'data_types': ['pii', 'financial']
    }
    
    classification = classifier.classify_incident(sample_incident)
    print(json.dumps(classification, indent=2, default=str))
```

### 4.2 Corr√©lateur d'alertes intelligent

![Correlation](https://img.shields.io/badge/Tool-Alert%20Correlator-purple)
![AI Enhanced](https://img.shields.io/badge/Technology-AI%20Enhanced-blue)

```python
#!/usr/bin/env python3
# scripts/alert_correlation.py

import json
import numpy as np
from datetime import datetime, timedelta
from collections import defaultdict, deque
import networkx as nx
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

class IntelligentAlertCorrelator:
    def __init__(self, elasticsearch_url="http://localhost:9200"):
        self.es_url = elasticsearch_url
        self.correlation_window = timedelta(hours=1)
        self.alert_buffer = deque(maxlen=10000)
        self.correlation_rules = self._load_correlation_rules()
        self.attack_patterns = self._initialize_attack_patterns()
        
    def _load_correlation_rules(self):
        """Chargement des r√®gles de corr√©lation"""
        return {
            # R√®gles pour campagnes coordonn√©es
            'coordinated_attack': {
                'conditions': [
                    {'type': 'brute_force_attack', 'min_sources': 3, 'time_window': 300},
                    {'type': 'port_scan', 'same_targets': True, 'time_window': 600},
                    {'type': 'web_attack', 'same_payload_family': True, 'time_window': 900}
                ],
                'correlation_threshold': 0.7,
                'severity_escalation': 'high'
            },
            
            # R√®gles pour attaques multi-√©tapes
            'multi_stage_attack': {
                'sequence': [
                    'reconnaissance',
                    'initial_access',
                    'privilege_escalation',
                    'lateral_movement',
                    'data_exfiltration'
                ],
                'max_time_between_stages': 3600,  # 1 heure
                'min_stages_matched': 3,
                'severity_escalation': 'critical'
            },
            
            # R√®gles pour attaques distribu√©es
            'distributed_attack': {
                'conditions': {
                    'min_unique_sources': 10,
                    'max_geographic_variance': 5000,  # km
                    'attack_synchronization': 0.8,
                    'time_window': 1800  # 30 minutes
                },
                'severity_escalation': 'high'
            },
            
            # R√®gles pour compromission d'infrastructure
            'infrastructure_compromise': {
                'indicators': [
                    'privileged_account_abuse',
                    'system_modification',
                    'network_configuration_change',
                    'security_tool_disable'
                ],
                'correlation_methods': ['temporal', 'source_based', 'target_based'],
                'severity_escalation': 'critical'
            }
        }
    
    def _initialize_attack_patterns(self):
        """Initialisation des patterns d'attaque connus"""
        return {
            'apt_patterns': {
                'lateral_movement': {
                    'sequence': ['reconnaissance', 'credential_theft', 'remote_access'],
                    'tools': ['psexec', 'wmic', 'powershell'],
                    'persistence': ['registry', 'scheduled_tasks', 'services']
                },
                'data_exfiltration': {
                    'preparation': ['data_discovery', 'data_staging', 'compression'],
                    'channels': ['dns_tunneling', 'https_upload', 'email_exfiltration']
                }
            },
            'automated_attacks': {
                'bot_campaigns': {
                    'characteristics': ['high_volume', 'distributed_sources', 'similar_payloads'],
                    'targets': ['login_pages', 'api_endpoints', 'contact_forms']
                },
                'vulnerability_exploitation': {
                    'phases': ['scanning', 'exploitation', 'payload_delivery'],
                    'indicators': ['cve_patterns', 'exploit_signatures', 'shell_commands']
                }
            }
        }
    
    def correlate_alerts(self, alerts):
        """Corr√©lation principale des alertes"""
        print(f"[+] Corr√©lation de {len(alerts)} alertes...")
        
        # Preprocessing des alertes
        processed_alerts = self._preprocess_alerts(alerts)
        
        # Application des diff√©rentes m√©thodes de corr√©lation
        correlations = {
            'temporal': self._temporal_correlation(processed_alerts),
            'spatial': self._spatial_correlation(processed_alerts),
            'behavioral': self._behavioral_correlation(processed_alerts),
            'pattern_based': self._pattern_based_correlation(processed_alerts),
            'ml_clustering': self._ml_based_correlation(processed_alerts)
        }
        
        # Fusion des corr√©lations
        merged_correlations = self._merge_correlations(correlations)
        
        # G√©n√©ration des incidents corr√©l√©s
        correlated_incidents = self._generate_correlated_incidents(merged_correlations)
        
        # √âvaluation de la confiance et priorisation
        prioritized_incidents = self._prioritize_correlated_incidents(correlated_incidents)
        
        return prioritized_incidents
    
    def _preprocess_alerts(self, alerts):
        """Pr√©processing et enrichissement des alertes"""
        processed = []
        
        for alert in alerts:
            # Extraction et normalisation des caract√©ristiques
            features = {
                'id': alert.get('id', self._generate_alert_id()),
                'timestamp': self._parse_timestamp(alert.get('timestamp')),
                'source_ip': alert.get('source_ip', ''),
                'target_ip': alert.get('target_ip', ''),
                'alert_type': alert.get('type', ''),
                'severity': alert.get('severity', 'medium'),
                'tags': alert.get('tags', []),
                'payload': alert.get('payload', ''),
                'user_agent': alert.get('user_agent', ''),
                'geographic_info': alert.get('geoip', {}),
                'network_info': self._extract_network_info(alert),
                'behavioral_info': self._extract_behavioral_info(alert)
            }
            
            # Enrichissement avec des donn√©es contextuelles
            features.update(self._enrich_alert_context(features))
            
            processed.append(features)
        
        return processed
    
    def _temporal_correlation(self, alerts):
        """Corr√©lation temporelle des alertes"""
        temporal_groups = defaultdict(list)
        
        # Regroupement par fen√™tres temporelles
        for alert in alerts:
            time_bucket = self._get_time_bucket(alert['timestamp'], bucket_size=300)  # 5 min
            temporal_groups[time_bucket].append(alert)
        
        correlations = []
        for time_bucket, group_alerts in temporal_groups.items():
            if len(group_alerts) >= 3:  # Minimum 3 alertes pour corr√©lation
                correlation = {
                    'type': 'temporal',
                    'time_window': time_bucket,
                    'alerts': group_alerts,
                    'correlation_score': self._calculate_temporal_score(group_alerts),
                    'characteristics': self._analyze_temporal_pattern(group_alerts)
                }
                correlations.append(correlation)
        
        return correlations
    
    def _spatial_correlation(self, alerts):
        """Corr√©lation spatiale (g√©ographique et r√©seau)"""
        spatial_groups = defaultdict(list)
        
        # Regroupement par source IP
        for alert in alerts:
            source_ip = alert['source_ip']
            if source_ip:
                # Regroupement par sous-r√©seau /24
                network = '.'.join(source_ip.split('.')[:3]) + '.0/24'
                spatial_groups[network].append(alert)
        
        correlations = []
        for network, group_alerts in spatial_groups.items():
            if len(group_alerts) >= 2:
                correlation = {
                    'type': 'spatial',
                    'network': network,
                    'alerts': group_alerts,
                    'correlation_score': self._calculate_spatial_score(group_alerts),
                    'geographic_analysis': self._analyze_geographic_distribution(group_alerts)
                }
                correlations.append(correlation)
        
        return correlations
    
    def _behavioral_correlation(self, alerts):
        """Corr√©lation comportementale"""
        behavioral_groups = defaultdict(list)
        
        # Regroupement par patterns comportementaux
        for alert in alerts:
            behavior_signature = self._generate_behavior_signature(alert)
            behavioral_groups[behavior_signature].append(alert)
        
        correlations = []
        for signature, group_alerts in behavioral_groups.items():
            if len(group_alerts) >= 2 and signature != 'unknown':
                correlation = {
                    'type': 'behavioral',
                    'behavior_signature': signature,
                    'alerts': group_alerts,
                    'correlation_score': self._calculate_behavioral_score(group_alerts),
                    'attack_pattern': self._identify_attack_pattern(group_alerts)
                }
                correlations.append(correlation)
        
        return correlations
    
    def _pattern_based_correlation(self, alerts):
        """Corr√©lation bas√©e sur des patterns d'attaque connus"""
        correlations = []
        
        # Recherche de patterns APT
        apt_correlation = self._detect_apt_patterns(alerts)
        if apt_correlation:
            correlations.append(apt_correlation)
        
        # Recherche de campagnes automatis√©es
        bot_correlation = self._detect_bot_campaigns(alerts)
        if bot_correlation:
            correlations.append(bot_correlation)
        
        # Recherche d'attaques multi-√©tapes
        multistage_correlation = self._detect_multistage_attacks(alerts)
        if multistage_correlation:
            correlations.append(multistage_correlation)
        
        return correlations
    
    def _ml_based_correlation(self, alerts):
        """Corr√©lation bas√©e sur l'apprentissage automatique"""
        if len(alerts) < 5:
            return []
        
        # Extraction des features pour clustering
        features_matrix = self._extract_ml_features(alerts)
        
        # Normalisation
        scaler = StandardScaler()
        features_normalized = scaler.fit_transform(features_matrix)
        
        # Clustering DBSCAN pour identifier les groupes d'alertes similaires
        clustering = DBSCAN(eps=0.5, min_samples=3)
        cluster_labels = clustering.fit_predict(features_normalized)
        
        # Cr√©ation des corr√©lations bas√©es sur les clusters
        correlations = []
        unique_clusters = set(cluster_labels)
        unique_clusters.discard(-1)  # Retirer les outliers
        
        for cluster_id in unique_clusters:
            cluster_alerts = [alerts[i] for i, label in enumerate(cluster_labels) if label == cluster_id]
            
            if len(cluster_alerts) >= 3:
                correlation = {
                    'type': 'ml_clustering',
                    'cluster_id': cluster_id,
                    'alerts': cluster_alerts,
                    'correlation_score': self._calculate_ml_correlation_score(cluster_alerts, features_matrix),
                    'cluster_characteristics': self._analyze_cluster_characteristics(cluster_alerts)
                }
                correlations.append(correlation)
        
        return correlations
    
    def _merge_correlations(self, correlation_dict):
        """Fusion des diff√©rents types de corr√©lations"""
        all_correlations = []
        
        # Collecte de toutes les corr√©lations
        for corr_type, correlations in correlation_dict.items():
            for correlation in correlations:
                correlation['method'] = corr_type
                all_correlations.append(correlation)
        
        # Fusion des corr√©lations qui se chevauchent
        merged = []
        processed_alerts = set()
        
        # Tri par score de corr√©lation d√©croissant
        all_correlations.sort(key=lambda x: x['correlation_score'], reverse=True)
        
        for correlation in all_correlations:
            alert_ids = {alert['id'] for alert in correlation['alerts']}
            
            # V√©rifier le chevauchement avec des corr√©lations d√©j√† trait√©es
            if not alert_ids.intersection(processed_alerts):
                merged.append(correlation)
                processed_alerts.update(alert_ids)
            else:
                # Tentative de fusion si chevauchement partiel significatif
                overlap_ratio = len(alert_ids.intersection(processed_alerts)) / len(alert_ids)
                if overlap_ratio < 0.5:  # Moins de 50% de chevauchement
                    # Cr√©er une nouvelle corr√©lation avec les alertes non trait√©es
                    new_alerts = [alert for alert in correlation['alerts'] 
                                if alert['id'] not in processed_alerts]
                    if len(new_alerts) >= 2:
                        new_correlation = correlation.copy()
                        new_correlation['alerts'] = new_alerts
                        new_correlation['correlation_score'] *= (1 - overlap_ratio)
                        merged.append(new_correlation)
                        processed_alerts.update({alert['id'] for alert in new_alerts})
        
        return merged
    
    def _generate_correlated_incidents(self, correlations):
        """G√©n√©ration d'incidents corr√©l√©s √† partir des corr√©lations"""
        incidents = []
        
        for correlation in correlations:
            incident = {
                'id': self._generate_incident_id(),
                'timestamp': datetime.now().isoformat(),
                'correlation_method': correlation['method'],
                'correlation_type': correlation['type'],
                'correlation_score': correlation['correlation_score'],
                'related_alerts': correlation['alerts'],
                'alert_count': len(correlation['alerts']),
                'time_span': self._calculate_time_span(correlation['alerts']),
                'affected_sources': self._extract_unique_sources(correlation['alerts']),
                'affected_targets': self._extract_unique_targets(correlation['alerts']),
                'severity': self._calculate_incident_severity(correlation),
                'confidence': self._calculate_correlation_confidence(correlation),
                'attack_vector': self._determine_attack_vector(correlation),
                'kill_chain_stage': self._identify_kill_chain_stage(correlation),
                'description': self._generate_incident_description(correlation),
                'recommended_response': self._suggest_incident_response(correlation)
            }
            
            incidents.append(incident)
        
        return incidents
    
    def _prioritize_correlated_incidents(self, incidents):
        """Priorisation des incidents corr√©l√©s"""
        # Calcul du score de priorit√© pour chaque incident
        for incident in incidents:
            priority_score = 0
            
            # Facteurs de priorit√©
            severity_weights = {'low': 1, 'medium': 3, 'high': 7, 'critical': 10}
            priority_score += severity_weights.get(incident['severity'], 3)
            
            # Score de corr√©lation
            priority_score += incident['correlation_score'] * 5
            
            # Nombre d'alertes impliqu√©es
            priority_score += min(incident['alert_count'] / 10, 3)
            
            # Confiance dans la corr√©lation
            priority_score += incident['confidence'] * 3
            
            # Facteurs d'aggravation
            if 'critical_infrastructure' in incident.get('affected_targets', []):
                priority_score += 5
            if incident.get('kill_chain_stage') in ['actions_on_objectives', 'data_exfiltration']:
                priority_score += 4
            
            incident['priority_score'] = priority_score
            incident['priority_level'] = self._assign_priority_level(priority_score)
        
        # Tri par score de priorit√©
        return sorted(incidents, key=lambda x: x['priority_score'], reverse=True)
    
    def generate_correlation_report(self, incidents):
        """G√©n√©ration d'un rapport de corr√©lation"""
        if not incidents:
            return "Aucune corr√©lation d√©tect√©e."
        
        # Analyses statistiques
        correlation_methods = defaultdict(int)
        severity_distribution = defaultdict(int)
        attack_vectors = defaultdict(int)
        
        total_alerts_correlated = sum(incident['alert_count'] for incident in incidents)
        avg_correlation_score = np.mean([incident['correlation_score'] for incident in incidents])
        
        for incident in incidents:
            correlation_methods[incident['correlation_method']] += 1
            severity_distribution[incident['severity']] += 1
            attack_vectors[incident['attack_vector']] += 1
        
        # G√©n√©ration du rapport
        report = f"""
![Correlation Report](https://img.shields.io/badge/Report-Alert%20Correlation-purple)
![Incidents](https://img.shields.io/badge/Correlated%20Incidents-{len(incidents)}-blue)
![Alerts](https://img.shields.io/badge/Total%20Alerts%20Correlated-{total_alerts_correlated}-orange)
![Avg Score](https://img.shields.io/badge/Avg%20Correlation%20Score-{avg_correlation_score:.2f}-green)

# üîó RAPPORT DE CORR√âLATION D'ALERTES

**P√©riode d'analyse**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Incidents corr√©l√©s**: {len(incidents)}  
**Total d'alertes corr√©l√©es**: {total_alerts_correlated}  
**Score de corr√©lation moyen**: {avg_correlation_score:.3f}

## üìä M√©thodes de corr√©lation utilis√©es

"""
        
        for method, count in correlation_methods.most_common():
            percentage = (count / len(incidents)) * 100
            report += f"- ![{method}](https://img.shields.io/badge/{method.replace('_', '%20')}-{count}%20({percentage:.1f}%%)-lightblue)\n"
        
        report += f"\n## üéØ Distribution par s√©v√©rit√©\n\n"
        
        for severity in ['critical', 'high', 'medium', 'low']:
            count = severity_distribution[severity]
            if count > 0:
                percentage = (count / len(incidents)) * 100
                color = {'critical': 'red', 'high': 'orange', 'medium': 'yellow', 'low': 'green'}[severity]
                report += f"- ![{severity}](https://img.shields.io/badge/Severity-{severity.upper()}-{color}) {count} incidents ({percentage:.1f}%)\n"
        
        report += f"\n## üéØ Vecteurs d'attaque identifi√©s\n\n"
        
        for vector, count in attack_vectors.most_common():
            percentage = (count / len(incidents)) * 100
            report += f"- **{vector.replace('_', ' ').title()}**: {count} incidents ({percentage:.1f}%)\n"
        
        report += f"\n## üö® Incidents corr√©l√©s prioritaires\n\n"
        
        # Top 10 des incidents les plus prioritaires
        for i, incident in enumerate(incidents[:10], 1):
            severity_color = {'critical': 'red', 'high': 'orange', 'medium': 'yellow', 'low': 'green'}.get(incident['severity'], 'grey')
            
            report += f"""
### {i}. Incident Corr√©l√© #{incident['id']}
![Priority](https://img.shields.io/badge/Priority-{incident['priority_level'].replace(' ', '%20')}-red)
![Severity](https://img.shields.io/badge/Severity-{incident['severity'].upper()}-{severity_color})
![Correlation](https://img.shields.io/badge/Correlation%20Score-{incident['correlation_score']:.2f}-blue)
![Confidence](https://img.shields.io/badge/Confidence-{incident['confidence']:.1%}-green)

**Description**: {incident['description']}  
**M√©thode de corr√©lation**: {incident['correlation_method']}  
**Alertes impliqu√©es**: {incident['alert_count']}  
**Dur√©e**: {incident['time_span']}  
**Vecteur d'attaque**: {incident['attack_vector']}  
**Phase kill chain**: {incident['kill_chain_stage']}

**Sources affect√©es**: {len(incident['affected_sources'])} IPs uniques  
**Cibles affect√©es**: {len(incident['affected_targets'])} syst√®mes  

**R√©ponse recommand√©e**: {incident['recommended_response']}

---
"""
        
        # Sauvegarde du rapport
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_filename = f"reports/correlation_report_{timestamp}.md"
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # Sauvegarde des donn√©es JSON
        json_filename = f"reports/correlation_data_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(incidents, f, indent=2, default=str)
        
        print(f"[+] Rapport de corr√©lation g√©n√©r√©: {report_filename}")
        print(f"[+] Donn√©es JSON sauvegard√©es: {json_filename}")
        
        return report, incidents
    
    # M√©thodes utilitaires pour la corr√©lation
    def _parse_timestamp(self, timestamp_str):
        """Parse et normalise les timestamps"""
        if isinstance(timestamp_str, str):
            return datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
        return timestamp_str or datetime.now()
    
    def _get_time_bucket(self, timestamp, bucket_size=300):
        """Cr√©ation de buckets temporels"""
        epoch = timestamp.timestamp()
        return int(epoch // bucket_size) * bucket_size
    
    def _generate_alert_id(self):
        """G√©n√©ration d'ID d'alerte unique"""
        return f"ALT-{datetime.now().strftime('%Y%m%d%H%M%S')}-{np.random.randint(1000, 9999)}"
    
    def _generate_incident_id(self):
        """G√©n√©ration d'ID d'incident corr√©l√© unique"""
        return f"CORR-{datetime.now().strftime('%Y%m%d%H%M%S')}-{np.random.randint(1000, 9999)}"
    
    def _extract_network_info(self, alert):
        """Extraction d'informations r√©seau"""
        return {
            'src_port': alert.get('src_port'),
            'dst_port': alert.get('dst_port'),
            'protocol': alert.get('protocol'),
            'bytes_transferred': alert.get('bytes', 0)
        }
    
    def _extract_behavioral_info(self, alert):
        """Extraction d'informations comportementales"""
        return {
            'request_frequency': alert.get('frequency', 1),
            'payload_size': len(alert.get('payload', '')),
            'is_encrypted': 'encrypted' in alert.get('tags', []),
            'uses_tor': 'tor' in alert.get('tags', [])
        }
    
    def _enrich_alert_context(self, features):
        """Enrichissement contextuel des alertes"""
        enrichment = {}
        
        # Classification de l'IP source
        if features['source_ip']:
            enrichment['ip_reputation'] = self._check_ip_reputation(features['source_ip'])
            enrichment['is_known_bad'] = enrichment['ip_reputation'] < 0.3
        
        # Analyse du payload
        if features['payload']:
            enrichment['payload_analysis'] = self._analyze_payload(features['payload'])
        
        return enrichment
    
    def _calculate_temporal_score(self, alerts):
        """Calcul du score de corr√©lation temporelle"""
        if len(alerts) < 2:
            return 0.0
        
        # Analyse de la synchronisation temporelle
        timestamps = [alert['timestamp'] for alert in alerts]
        time_deltas = [(timestamps[i+1] - timestamps[i]).total_seconds() 
                      for i in range(len(timestamps)-1)]
        
        # Score bas√© sur la r√©gularit√© des intervalles
        if time_deltas:
            avg_delta = np.mean(time_deltas)
            delta_variance = np.var(time_deltas)
            
            # Plus la variance est faible, plus le score est √©lev√©
            regularity_score = 1.0 / (1.0 + delta_variance / 100)
            
            # Bonus pour les attaques rapproch√©es dans le temps
            time_proximity_score = 1.0 / (1.0 + avg_delta / 300)  # 5 minutes de r√©f√©rence
            
            return min(1.0, (regularity_score + time_proximity_score) / 2)
        
        return 0.5
    
    def _calculate_spatial_score(self, alerts):
        """Calcul du score de corr√©lation spatiale"""
        unique_sources = len(set(alert['source_ip'] for alert in alerts if alert['source_ip']))
        total_alerts = len(alerts)
        
        if unique_sources == 0:
            return 0.0
        
        # Score bas√© sur la concentration des sources
        concentration_score = 1.0 / unique_sources if unique_sources > 0 else 1.0
        
        # Score bas√© sur le volume d'attaque
        volume_score = min(1.0, total_alerts / 10)
        
        return (concentration_score + volume_score) / 2
    
    def _generate_behavior_signature(self, alert):
        """G√©n√©ration d'une signature comportementale"""
        signature_elements = []
        
        # Type d'attaque
        signature_elements.append(alert['alert_type'])
        
        # Patterns dans le payload
        payload = alert.get('payload', '').lower()
        if 'union' in payload and 'select' in payload:
            signature_elements.append('sql_injection')
        if 'script' in payload:
            signature_elements.append('xss')
        if '../' in payload:
            signature_elements.append('path_traversal')
        
        # User Agent patterns
        user_agent = alert.get('user_agent', '').lower()
        if 'bot' in user_agent or 'crawler' in user_agent:
            signature_elements.append('automated_tool')
        if 'curl' in user_agent or 'wget' in user_agent:
            signature_elements.append('command_line_tool')
        
        return '_'.join(signature_elements) if signature_elements else 'unknown'
    
    def _calculate_behavioral_score(self, alerts):
        """Calcul du score de corr√©lation comportementale"""
        if len(alerts) < 2:
            return 0.0
        
        # Analyse de la similarit√© comportementale
        behavior_elements = defaultdict(int)
        
        for alert in alerts:
            signature = self._generate_behavior_signature(alert)
            behavior_elements[signature] += 1
        
        # Score bas√© sur la coh√©rence comportementale
        max_similar = max(behavior_elements.values())
        consistency_score = max_similar / len(alerts)
        
        return consistency_score
    
    def _detect_apt_patterns(self, alerts):
        """D√©tection de patterns APT"""
        # Recherche de s√©quences caract√©ristiques d'APT
        apt_stages = ['reconnaissance', 'initial_access', 'persistence', 'privilege_escalation', 
                     'lateral_movement', 'data_collection', 'exfiltration']
        
        detected_stages = set()
        stage_alerts = defaultdict(list)
        
        for alert in alerts:
            for stage in apt_stages:
                if any(keyword in ' '.join(alert.get('tags', [])) for keyword in self._get_stage_keywords(stage)):
                    detected_stages.add(stage)
                    stage_alerts[stage].append(alert)
        
        # Si au moins 3 stages sont d√©tect√©s, consid√©rer comme APT
        if len(detected_stages) >= 3:
            all_apt_alerts = []
            for alerts_list in stage_alerts.values():
                all_apt_alerts.extend(alerts_list)
            
            return {
                'type': 'apt_pattern',
                'alerts': all_apt_alerts,
                'detected_stages': list(detected_stages),
                'correlation_score': len(detected_stages) / len(apt_stages),
                'attack_sophistication': 'high'
            }
        
        return None
    
    def _detect_bot_campaigns(self, alerts):
        """D√©tection de campagnes de bots"""
        # Groupement par caract√©ristiques de bot
        bot_characteristics = defaultdict(list)
        
        for alert in alerts:
            # Identification des caract√©ristiques de bot
            characteristics = []
            
            user_agent = alert.get('user_agent', '').lower()
            if any(bot_indicator in user_agent for bot_indicator in ['bot', 'crawler', 'spider', 'scraper']):
                characteristics.append('bot_user_agent')
